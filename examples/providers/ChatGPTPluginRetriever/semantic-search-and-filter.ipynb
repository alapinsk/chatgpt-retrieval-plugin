{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import AIPluginTool\n",
    "from langchain.tools.plugin import AIPlugin\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_VERSION = \"2023-03-15-preview\"\n",
    "OPENAI_MODEL_NAME =  \"gpt-35-turbo-0301\"  # os.getenv(\"OPENAI_COMPLETIONMODEL_DEPLOYMENTID\")\n",
    "PLUGIN_ENDPOINT_URL = \"http://0.0.0.0:3333\"\n",
    "PLUGIN_BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n",
    "PLUGIN_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {PLUGIN_BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "openai.api_type = os.getenv('OPENAI_API_TYPE')\n",
    "openai.api_version = OPENAI_API_VERSION\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject list at 0x11bda6340> JSON: {\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"text-embedding-ada-002\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"text-embedding-ada-002\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1678902936,\n",
       "      \"updated_at\": 1678902936,\n",
       "      \"object\": \"deployment\"\n",
       "    },\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"gpt-35-turbo\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"gpt-35-turbo-0301\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1680520260,\n",
       "      \"updated_at\": 1689939708,\n",
       "      \"object\": \"deployment\"\n",
       "    },\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"text-embedding-ada-002\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"text-embedding-ada-002-v2\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1683751121,\n",
       "      \"updated_at\": 1683751121,\n",
       "      \"object\": \"deployment\"\n",
       "    },\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"text-davinci-003\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"text-davinci-003-1\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1684485361,\n",
       "      \"updated_at\": 1684485361,\n",
       "      \"object\": \"deployment\"\n",
       "    },\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"gpt-35-turbo-16k\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"gpt-35-turbo-16k\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1688995318,\n",
       "      \"updated_at\": 1689939605,\n",
       "      \"object\": \"deployment\"\n",
       "    },\n",
       "    {\n",
       "      \"scale_settings\": {\n",
       "        \"scale_type\": \"standard\"\n",
       "      },\n",
       "      \"model\": \"gpt-35-turbo\",\n",
       "      \"owner\": \"organization-owner\",\n",
       "      \"id\": \"gpt-35-turbo-0613\",\n",
       "      \"status\": \"succeeded\",\n",
       "      \"created_at\": 1689942531,\n",
       "      \"updated_at\": 1689942531,\n",
       "      \"object\": \"deployment\"\n",
       "    }\n",
       "  ],\n",
       "  \"object\": \"list\"\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Deployment.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIPluginTool(name='retrieval', description='Call this tool to get the OpenAPI spec (and usage guide) for interacting with the Retrieval Plugin API. You should only call this ONCE! What is the Retrieval Plugin API useful for? Search through your documents.', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, plugin=AIPlugin(schema_version='v1', name_for_model='retrieval', name_for_human='Retrieval Plugin', description_for_model=\"Plugin for searching through the user's documents (such as files, emails, phone numbers and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.\", description_for_human='Search through your documents.', auth={'type': 'none'}, api=ApiConfig(type='openapi', url='http://localhost:3333/.well-known/openapi.yaml', has_user_authentication=False), logo_url='http://localhost:3333/.well-known/logo.png', contact_email='hello@contact.com', legal_info_url='hello@legal.com'), api_spec='Usage Guide: Plugin for searching through the user\\'s documents (such as files, emails, phone numbers and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.\\n\\nOpenAPI Spec: {\\'openapi\\': \\'3.0.2\\', \\'info\\': {\\'title\\': \\'Retrieval Plugin API\\', \\'description\\': \\'A retrieval API for querying and filtering documents based on natural language queries and metadata\\', \\'version\\': \\'1.0.0\\', \\'servers\\': [{\\'url\\': \\'http://localhost:3333\\'}]}, \\'paths\\': {\\'/query\\': {\\'post\\': {\\'summary\\': \\'Query\\', \\'description\\': \"Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don\\'t do this often. Split queries if ResponseTooLargeError occurs.\", \\'operationId\\': \\'query_query_post\\', \\'requestBody\\': {\\'content\\': {\\'application/json\\': {\\'schema\\': {\\'$ref\\': \\'#/components/schemas/QueryRequest\\'}}}, \\'required\\': True}, \\'responses\\': {\\'200\\': {\\'description\\': \\'Successful Response\\', \\'content\\': {\\'application/json\\': {\\'schema\\': {\\'$ref\\': \\'#/components/schemas/QueryResponse\\'}}}}, \\'422\\': {\\'description\\': \\'Validation Error\\', \\'content\\': {\\'application/json\\': {\\'schema\\': {\\'$ref\\': \\'#/components/schemas/HTTPValidationError\\'}}}}}}}}, \\'components\\': {\\'schemas\\': {\\'DocumentChunkMetadata\\': {\\'title\\': \\'DocumentChunkMetadata\\', \\'type\\': \\'object\\', \\'properties\\': {\\'source\\': {\\'$ref\\': \\'#/components/schemas/Source\\'}, \\'source_id\\': {\\'title\\': \\'Source Id\\', \\'type\\': \\'string\\'}, \\'url\\': {\\'title\\': \\'Url\\', \\'type\\': \\'string\\'}, \\'created_at\\': {\\'title\\': \\'Created At\\', \\'type\\': \\'string\\'}, \\'author\\': {\\'title\\': \\'Author\\', \\'type\\': \\'string\\'}, \\'document_id\\': {\\'title\\': \\'Document Id\\', \\'type\\': \\'string\\'}}}, \\'DocumentChunkWithScore\\': {\\'title\\': \\'DocumentChunkWithScore\\', \\'required\\': [\\'text\\', \\'metadata\\', \\'score\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'id\\': {\\'title\\': \\'Id\\', \\'type\\': \\'string\\'}, \\'text\\': {\\'title\\': \\'Text\\', \\'type\\': \\'string\\'}, \\'metadata\\': {\\'$ref\\': \\'#/components/schemas/DocumentChunkMetadata\\'}, \\'embedding\\': {\\'title\\': \\'Embedding\\', \\'type\\': \\'array\\', \\'items\\': {\\'type\\': \\'number\\'}}, \\'score\\': {\\'title\\': \\'Score\\', \\'type\\': \\'number\\'}}}, \\'DocumentMetadataFilter\\': {\\'title\\': \\'DocumentMetadataFilter\\', \\'type\\': \\'object\\', \\'properties\\': {\\'document_id\\': {\\'title\\': \\'Document Id\\', \\'type\\': \\'string\\'}, \\'source\\': {\\'$ref\\': \\'#/components/schemas/Source\\'}, \\'source_id\\': {\\'title\\': \\'Source Id\\', \\'type\\': \\'string\\'}, \\'author\\': {\\'title\\': \\'Author\\', \\'type\\': \\'string\\'}, \\'start_date\\': {\\'title\\': \\'Start Date\\', \\'type\\': \\'string\\'}, \\'end_date\\': {\\'title\\': \\'End Date\\', \\'type\\': \\'string\\'}}}, \\'HTTPValidationError\\': {\\'title\\': \\'HTTPValidationError\\', \\'type\\': \\'object\\', \\'properties\\': {\\'detail\\': {\\'title\\': \\'Detail\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/components/schemas/ValidationError\\'}}}}, \\'Query\\': {\\'title\\': \\'Query\\', \\'required\\': [\\'query\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'query\\': {\\'title\\': \\'Query\\', \\'type\\': \\'string\\'}, \\'filter\\': {\\'$ref\\': \\'#/components/schemas/DocumentMetadataFilter\\'}, \\'top_k\\': {\\'title\\': \\'Top K\\', \\'type\\': \\'integer\\', \\'default\\': 3}}}, \\'QueryRequest\\': {\\'title\\': \\'QueryRequest\\', \\'required\\': [\\'queries\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'queries\\': {\\'title\\': \\'Queries\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/components/schemas/Query\\'}}}}, \\'QueryResponse\\': {\\'title\\': \\'QueryResponse\\', \\'required\\': [\\'results\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'results\\': {\\'title\\': \\'Results\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/components/schemas/QueryResult\\'}}}}, \\'QueryResult\\': {\\'title\\': \\'QueryResult\\', \\'required\\': [\\'query\\', \\'results\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'query\\': {\\'title\\': \\'Query\\', \\'type\\': \\'string\\'}, \\'results\\': {\\'title\\': \\'Results\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/components/schemas/DocumentChunkWithScore\\'}}}}, \\'Source\\': {\\'title\\': \\'Source\\', \\'enum\\': [\\'email\\', \\'file\\', \\'chat\\'], \\'type\\': \\'string\\', \\'description\\': \\'An enumeration.\\'}, \\'ValidationError\\': {\\'title\\': \\'ValidationError\\', \\'required\\': [\\'loc\\', \\'msg\\', \\'type\\'], \\'type\\': \\'object\\', \\'properties\\': {\\'loc\\': {\\'title\\': \\'Location\\', \\'type\\': \\'array\\', \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'integer\\'}]}}, \\'msg\\': {\\'title\\': \\'Message\\', \\'type\\': \\'string\\'}, \\'type\\': {\\'title\\': \\'Error Type\\', \\'type\\': \\'string\\'}}}}}}')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool = AIPluginTool.from_plugin_url(f\"{PLUGIN_ENDPOINT_URL}/.well-known/ai-plugin.json\")\n",
    "tools = [tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'I want information related to car dealerships.',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.795724915277784},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7079306278833792},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.6948336097185607}]}]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"I want information related to car dealerships.\",\n",
    "    \"filter\": {\"source_id\": \"test:*\", \"author\": \"Vape Jordan\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{PLUGIN_ENDPOINT_URL}/query\",\n",
    "    headers=PLUGIN_HEADERS,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(deployment_name=OPENAI_MODEL_NAME, openai_api_version=OPENAI_API_VERSION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to check a specific website to see if there are any articles written.\n",
      "Action: requests_get\n",
      "Action Input: https://www.examplewebsite.com/articles\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m<html><head><title>www.examplewebsite.com</title></head><body><h1>www.examplewebsite.com</h1><p>Coming soon.</p></body></html>\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems that the website is not yet live.\n",
      "Action: None\n",
      "Final Answer: Unable to determine if there are any articles written as the website is not live.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unable to determine if there are any articles written as the website is not live.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "agent_chain.run(\"Do we have any articles writte?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the Klarna Shopping API to search for t-shirts.\n",
      "Action: KlarnaProducts\n",
      "Action Input: None\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mUsage Guide: Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it's product expertise to provide information pertaining to the subject of the user's request that may guide them in their search for the right product.\n",
      "\n",
      "OpenAPI Spec: {'openapi': '3.0.1', 'info': {'version': 'v0', 'title': 'Open AI Klarna product Api'}, 'servers': [{'url': 'https://www.klarna.com/us/shopping'}], 'tags': [{'name': 'open-ai-product-endpoint', 'description': 'Open AI Product Endpoint. Query for products.'}], 'paths': {'/public/openai/v0/products': {'get': {'tags': ['open-ai-product-endpoint'], 'summary': 'API for fetching Klarna product information', 'operationId': 'productsUsingGET', 'parameters': [{'name': 'countryCode', 'in': 'query', 'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.', 'required': True, 'schema': {'type': 'string'}}, {'name': 'q', 'in': 'query', 'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\", 'required': True, 'schema': {'type': 'string'}}, {'name': 'size', 'in': 'query', 'description': 'number of products returned', 'required': False, 'schema': {'type': 'integer'}}, {'name': 'min_price', 'in': 'query', 'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\", 'required': False, 'schema': {'type': 'integer'}}, {'name': 'max_price', 'in': 'query', 'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\", 'required': False, 'schema': {'type': 'integer'}}], 'responses': {'200': {'description': 'Products found', 'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ProductResponse'}}}}, '503': {'description': 'one or more services are unavailable'}}, 'deprecated': False}}}, 'components': {'schemas': {'Product': {'type': 'object', 'properties': {'attributes': {'type': 'array', 'items': {'type': 'string'}}, 'name': {'type': 'string'}, 'price': {'type': 'string'}, 'url': {'type': 'string'}}, 'title': 'Product'}, 'ProductResponse': {'type': 'object', 'properties': {'products': {'type': 'array', 'items': {'$ref': '#/components/schemas/Product'}}}, 'title': 'ProductResponse'}}}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use the GET request to search for t-shirts on the Klarna Shopping API.\n",
      "Action: requests_get\n",
      "Action Input: 'https://www.klarna.com/us/shopping/public/openai/v0/products?countryCode=US&q=t-shirt&size=10'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{\"products\":[{\"name\":\"adidas Adicolor Classics Trefoil T-shirt - Black/White\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201095924/Clothing/adidas-Adicolor-Classics-Trefoil-T-shirt-Black-White/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$11.22\",\"attributes\":[\"Material:Jersey,Cotton\",\"Target Group:Man\",\"Color:Black\",\"Size:S,XL,XS,L,M,XXL\"]},{\"name\":\"Polo Ralph Lauren Slim Fit Cotton T-shirt 3-pack\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201838628/Clothing/Polo-Ralph-Lauren-Slim-Fit-Cotton-T-shirt-3-pack/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$31.08\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,White,Blue,Multicolor,Black\",\"Pattern:Solid Color\",\"Fit:Slim\",\"Size:S,XL,XS,L,M,XXL\"]},{\"name\":\"Armani Exchange Men's Script Logo T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3202187360/Clothing/Armani-Exchange-Men-s-Script-Logo-T-shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$28.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Blue,Black\",\"Size:S,XL,XS,L,M,XXL\"]},{\"name\":\"Tommy Hilfiger Kid's Flag T-Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl359/3202370182/Children-s-Clothing/Tommy-Hilfiger-Kid-s-Flag-T-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$9.80\",\"attributes\":[\"Material:Polyester,Cotton,Organic Cotton\",\"Color:Red,Pink,White,Yellow\",\"Model:Boy,Girl\",\"Pattern:Solid Color\",\"Size (Small-Large):S,XL,L,M\",\"Sustainability Attributes :Organic\",\"Size (US):12,14,16,7,8,10\"]},{\"name\":\"Psycho Bunny Mens Starwood Embroidered Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663246/Clothing/Psycho-Bunny-Mens-Starwood-Embroidered-Graphic-Tee/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$37.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Pink,Blue,Black\",\"Size:XXS,S,XL,3XL,XS,L,M,XXL\"]},{\"name\":\"Psycho Bunny Mens Copa Gradient Logo Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$60.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Blue,Black,Orange\",\"Size:XXS,S,XL,3XL,XS,L,M,XXL\"]},{\"name\":\"Gildan Softstyle T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3205025255/Clothing/Gildan-Softstyle-T-shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$2.99\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Red,Gold,Gray,White,Blue,Yellow,Purple,Beige,Black,Orange,Green\",\"Pattern:Solid Color\",\"Size:S,XL,3XL,XS,L,M,XXL\"]},{\"name\":\"Palm Angels Bear T-shirt - Black\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$160.13\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Black\",\"Size:S,XL,L,M\"]},{\"name\":\"Balmain White Printed T-Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3205123494/Clothing/Balmain-White-Printed-T-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$137.41\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man,Woman\",\"Color:White,Black\",\"Size:XXS,S,XL,XS,L,M\",\"Sustainability Attributes :Organic\"]},{\"name\":\"Amiri Core Logo T Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3206578052/Clothing/Amiri-Core-Logo-T-Shirt/?utm_source=openai&ref-site=openai_plugin\",\"price\":\"$254.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Black\",\"Size:S,XL,XS,L,M,XXL\"]}]}\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `The GET request to the Klarna Shopping API returned a list of t-shirts available for purchase.\nAction: None`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      6\u001b[0m agent_chain \u001b[39m=\u001b[39m initialize_agent(\n\u001b[1;32m      7\u001b[0m     tools, llm, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     handle_parsing_errors\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_k_below_max_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m agent_chain\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mwhat t shirts are available in klarna?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:796\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 796\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    797\u001b[0m         name_to_tool_map, color_mapping, inputs, intermediate_steps\n\u001b[1;32m    798\u001b[0m     )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    800\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:676\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \n\u001b[1;32m    673\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(intermediate_steps, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    677\u001b[0m \u001b[39m# If the tool chosen is the finishing tool, then we end and return.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, AgentFinish):\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:385\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    384\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:26\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msearch(regex, text, re\u001b[39m.\u001b[39mDOTALL)\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m match:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m action \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     28\u001b[0m action_input \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `The GET request to the Klarna Shopping API returned a list of t-shirts available for purchase.\nAction: None`"
     ]
    }
   ],
   "source": [
    "tools = load_tools([\"requests_all\"])\n",
    "tool = AIPluginTool.from_plugin_url(\"https://www.klarna.com/.well-known/ai-plugin.json\")\n",
    "tools += [tool]\n",
    "import json\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,\n",
    "    handle_parsing_errors=True, reduce_k_below_max_tokens=True\n",
    ")\n",
    "agent_chain.run(\"what t shirts are available in klarna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestsGetTool(name='requests_get', description='A portal to the internet. Use this when you need to get specific content from a website. Input should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request.', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, requests_wrapper=TextRequestsWrapper(headers=None, aiosession=None)),\n",
       " RequestsPostTool(name='requests_post', description='Use this when you want to POST to a website.\\n    Input should be a json string with two keys: \"url\" and \"data\".\\n    The value of \"url\" should be a string, and the value of \"data\" should be a dictionary of \\n    key-value pairs you want to POST to the url.\\n    Be careful to always use double quotes for strings in the json string\\n    The output will be the text response of the POST request.\\n    ', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, requests_wrapper=TextRequestsWrapper(headers=None, aiosession=None)),\n",
       " RequestsPatchTool(name='requests_patch', description='Use this when you want to PATCH to a website.\\n    Input should be a json string with two keys: \"url\" and \"data\".\\n    The value of \"url\" should be a string, and the value of \"data\" should be a dictionary of \\n    key-value pairs you want to PATCH to the url.\\n    Be careful to always use double quotes for strings in the json string\\n    The output will be the text response of the PATCH request.\\n    ', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, requests_wrapper=TextRequestsWrapper(headers=None, aiosession=None)),\n",
       " RequestsPutTool(name='requests_put', description='Use this when you want to PUT to a website.\\n    Input should be a json string with two keys: \"url\" and \"data\".\\n    The value of \"url\" should be a string, and the value of \"data\" should be a dictionary of \\n    key-value pairs you want to PUT to the url.\\n    Be careful to always use double quotes for strings in the json string.\\n    The output will be the text response of the PUT request.\\n    ', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, requests_wrapper=TextRequestsWrapper(headers=None, aiosession=None)),\n",
       " RequestsDeleteTool(name='requests_delete', description='A portal to the internet. Use this when you need to make a DELETE request to a URL. Input should be a specific url, and the output will be the text response of the DELETE request.', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, requests_wrapper=TextRequestsWrapper(headers=None, aiosession=None)),\n",
       " AIPluginTool(name='KlarnaProducts', description='Call this tool to get the OpenAPI spec (and usage guide) for interacting with the Klarna Shopping API. You should only call this ONCE! What is the Klarna Shopping API useful for? Search and compare prices from thousands of online shops. Only available in the US.', args_schema=None, return_direct=False, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x10ed4de40>, plugin=AIPlugin(schema_version='v1', name_for_model='KlarnaProducts', name_for_human='Klarna Shopping', description_for_model='Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it\\'s product expertise to provide information pertaining to the subject of the user\\'s request that may guide them in their search for the right product.', description_for_human='Search and compare prices from thousands of online shops. Only available in the US.', auth={'type': 'none'}, api=ApiConfig(type='openapi', url='https://www.klarna.com/us/shopping/public/openai/v0/api-docs/', has_user_authentication=False), logo_url='https://www.klarna.com/assets/sites/5/2020/04/27143923/klarna-K-150x150.jpg', contact_email='openai-products@klarna.com', legal_info_url='https://www.klarna.com/us/legal/'), api_spec='Usage Guide: Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose. Assistant will reply with the following 3 paragraphs 1) Search Results 2) Product Comparison of the Search Results 3) Followup Questions. The first paragraph contains a list of the products with their attributes listed clearly and concisely as bullet points under the product, together with a link to the product and an explanation. Links will always be returned and should be shown to the user. The second paragraph compares the results returned in a summary sentence starting with \"In summary\". Assistant comparisons consider only the most important features of the products that will help them fit the users request, and each product mention is brief, short and concise. In the third paragraph assistant always asks helpful follow-up questions and end with a question mark. When assistant is asking a follow-up question, it uses it\\'s product expertise to provide information pertaining to the subject of the user\\'s request that may guide them in their search for the right product.\\n\\nOpenAPI Spec: {\\'openapi\\': \\'3.0.1\\', \\'info\\': {\\'version\\': \\'v0\\', \\'title\\': \\'Open AI Klarna product Api\\'}, \\'servers\\': [{\\'url\\': \\'https://www.klarna.com/us/shopping\\'}], \\'tags\\': [{\\'name\\': \\'open-ai-product-endpoint\\', \\'description\\': \\'Open AI Product Endpoint. Query for products.\\'}], \\'paths\\': {\\'/public/openai/v0/products\\': {\\'get\\': {\\'tags\\': [\\'open-ai-product-endpoint\\'], \\'summary\\': \\'API for fetching Klarna product information\\', \\'operationId\\': \\'productsUsingGET\\', \\'parameters\\': [{\\'name\\': \\'countryCode\\', \\'in\\': \\'query\\', \\'description\\': \\'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.\\', \\'required\\': True, \\'schema\\': {\\'type\\': \\'string\\'}}, {\\'name\\': \\'q\\', \\'in\\': \\'query\\', \\'description\\': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don\\'t contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\", \\'required\\': True, \\'schema\\': {\\'type\\': \\'string\\'}}, {\\'name\\': \\'size\\', \\'in\\': \\'query\\', \\'description\\': \\'number of products returned\\', \\'required\\': False, \\'schema\\': {\\'type\\': \\'integer\\'}}, {\\'name\\': \\'min_price\\', \\'in\\': \\'query\\', \\'description\\': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for.\", \\'required\\': False, \\'schema\\': {\\'type\\': \\'integer\\'}}, {\\'name\\': \\'max_price\\', \\'in\\': \\'query\\', \\'description\\': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user\\'s request and the kind of product searched for.\", \\'required\\': False, \\'schema\\': {\\'type\\': \\'integer\\'}}], \\'responses\\': {\\'200\\': {\\'description\\': \\'Products found\\', \\'content\\': {\\'application/json\\': {\\'schema\\': {\\'$ref\\': \\'#/components/schemas/ProductResponse\\'}}}}, \\'503\\': {\\'description\\': \\'one or more services are unavailable\\'}}, \\'deprecated\\': False}}}, \\'components\\': {\\'schemas\\': {\\'Product\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'attributes\\': {\\'type\\': \\'array\\', \\'items\\': {\\'type\\': \\'string\\'}}, \\'name\\': {\\'type\\': \\'string\\'}, \\'price\\': {\\'type\\': \\'string\\'}, \\'url\\': {\\'type\\': \\'string\\'}}, \\'title\\': \\'Product\\'}, \\'ProductResponse\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'products\\': {\\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/components/schemas/Product\\'}}}, \\'title\\': \\'ProductResponse\\'}}}}')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/agents/custom_agent_with_plugin_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import (\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    "    LLMSingleActionAgent,\n",
    "    AgentOutputParser,\n",
    ")\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.agents.agent_toolkits import NLAToolkit\n",
    "from langchain.tools.plugin import AIPlugin\n",
    "import re\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://datasette.io/.well-known/ai-plugin.json\",\n",
    "    \"https://api.speak.com/.well-known/ai-plugin.json\",\n",
    "    \"https://www.wolframalpha.com/.well-known/ai-plugin.json\",\n",
    "    \"https://www.zapier.com/.well-known/ai-plugin.json\",\n",
    "    # \"https://www.klarna.com/.well-known/ai-plugin.json\",\n",
    "    \"https://www.joinmilo.com/.well-known/ai-plugin.json\",\n",
    "    \"https://slack.com/.well-known/ai-plugin.json\",\n",
    "    \"https://schooldigger.com/.well-known/ai-plugin.json\",\n",
    "]\n",
    "\n",
    "AI_PLUGINS = [AIPlugin.from_url(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.2 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n",
      "Attempting to load a Swagger 2.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(deployment=\"text-embedding-ada-002-v2\")\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=plugin.description_for_model,\n",
    "        metadata={\"plugin_name\": plugin.name_for_model},\n",
    "    )\n",
    "    for plugin in AI_PLUGINS\n",
    "]\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "toolkits_dict = {\n",
    "    plugin.name_for_model: NLAToolkit.from_llm_and_ai_plugin(llm, plugin)\n",
    "    for plugin in AI_PLUGINS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "\n",
    "def get_tools(query):\n",
    "    # Get documents, which contain the Plugins to use\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    # Get the toolkits, one for each plugin\n",
    "    tool_kits = [toolkits_dict[d.metadata[\"plugin_name\"]] for d in docs]\n",
    "    # Get the tools: a separate NLAChain for each endpoint\n",
    "    tools = []\n",
    "    for tk in tool_kits:\n",
    "        tools.extend(tk.nla_tools)\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Milo.askMilo',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.list_guided_recipes',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.search_all_actions',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.preview_a_zap',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.get_configuration_link',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.list_exposed_actions',\n",
       " 'Zapier_Natural_Language_Actions_(NLA)_API_(Dynamic)_-_Beta.get_execution_log_endpoint',\n",
       " 'SchoolDigger_API_V2.0.Autocomplete_GetSchools',\n",
       " 'SchoolDigger_API_V2.0.Districts_GetAllDistricts2',\n",
       " 'SchoolDigger_API_V2.0.Districts_GetDistrict2',\n",
       " 'SchoolDigger_API_V2.0.Rankings_GetSchoolRank2',\n",
       " 'SchoolDigger_API_V2.0.Rankings_GetRank_District',\n",
       " 'SchoolDigger_API_V2.0.Schools_GetAllSchools20',\n",
       " 'SchoolDigger_API_V2.0.Schools_GetSchool20',\n",
       " 'Speak.translate',\n",
       " 'Speak.explainPhrase',\n",
       " 'Speak.explainTask']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = get_tools(\"What could I do today with my kiddo\")\n",
    "[t.name for t in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Open_AI_Klarna_product_Api.productsUsingGET',\n",
       " 'Milo.askMilo',\n",
       " 'SchoolDigger_API_V2.0.Autocomplete_GetSchools',\n",
       " 'SchoolDigger_API_V2.0.Districts_GetAllDistricts2',\n",
       " 'SchoolDigger_API_V2.0.Districts_GetDistrict2',\n",
       " 'SchoolDigger_API_V2.0.Rankings_GetSchoolRank2',\n",
       " 'SchoolDigger_API_V2.0.Rankings_GetRank_District',\n",
       " 'SchoolDigger_API_V2.0.Schools_GetAllSchools20',\n",
       " 'SchoolDigger_API_V2.0.Schools_GetSchool20',\n",
       " 'Slack_AI_Plugin.ai_alpha_search_messages']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = get_tools(\"what shirts can i buy?\")\n",
    "[t.name for t in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    ############## NEW ######################\n",
    "    # The list of tools available\n",
    "    tools_getter: Callable\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        ############## NEW ######################\n",
    "        tools = self.tools_getter(kwargs[\"input\"])\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join(\n",
    "            [f\"{tool.name}: {tool.description}\" for tool in tools]\n",
    "        )\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in tools])\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools_getter=get_tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(\n",
    "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Ahoy, me hearties! This be a question about buying shirts. We'll need to use an API that fetches product information.\n",
      "Action: Open_AI_Klarna_product_Api.productsUsingGET\n",
      "Action Input: \"shirts\"\u001b[0m\n",
      "\n",
      "Observation:\u001b[36;1m\u001b[1;3mHere are some shirts you might be interested in:\n",
      "- Burberry SS Check Stretch Cotton Shirt - Archive Beige\n",
      "- Burberry Somerton Check Stretch Cotton Shirt - Black/Red/Beige\n",
      "- Calvin Klein Slim Fit Oxford Dress Shirt\n",
      "- Casablanca Pink Rainbow Monogram Shirt\n",
      "- Van Heusen Men's Dress Shirt Regular Fit\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mArrr, we've got a list of shirts to choose from. But let's see if we can narrow it down by searching for a specific brand or color.\n",
      "Action: Open_AI_Klarna_product_Api.productsUsingGET\n",
      "Action Input: \"Burberry black shirt\"\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No response found in output: {\"response\": \"One possible product matching the description 'Burberry black shirt' is the Burberry Harriston Logo T-shirt - Black, which is an oversize black t-shirt with the Burberry logo on it. It is priced at $325.93 and comes in sizes XXS, S, XL, XS, L, and M. However, there could be other products that match this description in the API_RESPONSE that are not included in this response.\"}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_executor\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mwhat shirts can i buy?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:796\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 796\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    797\u001b[0m         name_to_tool_map, color_mapping, inputs, intermediate_steps\n\u001b[1;32m    798\u001b[0m     )\n\u001b[1;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    800\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:699\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[1;32m    697\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    700\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    701\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    702\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m    703\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m    704\u001b[0m     )\n\u001b[1;32m    705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/tools/base.py:205\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_error(e, verbose\u001b[39m=\u001b[39mverbose_)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    207\u001b[0m     \u001b[39mstr\u001b[39m(observation), verbose\u001b[39m=\u001b[39mverbose_, color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m observation\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/tools/base.py:202\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(tool_input)\n\u001b[0;32m--> 202\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_error(e, verbose\u001b[39m=\u001b[39mverbose_)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/agents/tools.py:49\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/api/openapi/chain.py:148\u001b[0m, in \u001b[0;36mOpenAPIEndpointChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_text(\n\u001b[1;32m    145\u001b[0m     response_text, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_response_chain \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     _answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_response_chain\u001b[39m.\u001b[39;49mpredict_and_parse(\n\u001b[1;32m    149\u001b[0m         response\u001b[39m=\u001b[39;49mresponse_text,\n\u001b[1;32m    150\u001b[0m         instructions\u001b[39m=\u001b[39;49minstructions,\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     answer \u001b[39m=\u001b[39m cast(\u001b[39mstr\u001b[39m, _answer)\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_text(\n\u001b[1;32m    154\u001b[0m         answer, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myellow\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    155\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:173\u001b[0m, in \u001b[0;36mLLMChain.predict_and_parse\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt\u001b[39m.\u001b[39moutput_parser \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprompt\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(result)\n\u001b[1;32m    174\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Code/chatgpt-retrieval-plugin/.venv/lib/python3.10/site-packages/langchain/chains/api/openapi/response_chain.py:31\u001b[0m, in \u001b[0;36mAPIResponderOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_json_block(json_match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mstrip())\n\u001b[1;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo response found in output: \u001b[39m\u001b[39m{\u001b[39;00mllm_output\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No response found in output: {\"response\": \"One possible product matching the description 'Burberry black shirt' is the Burberry Harriston Logo T-shirt - Black, which is an oversize black t-shirt with the Burberry logo on it. It is priced at $325.93 and comes in sizes XXS, S, XL, XS, L, and M. However, there could be other products that match this description in the API_RESPONSE that are not included in this response.\"}."
     ]
    }
   ],
   "source": [
    "agent_executor.run(\"what shirts can i buy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "\n",
    "\n",
    "# Initialize gpt-35-turbo and our embedding model\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"requests_all\"])\n",
    "tools += [tool]\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "agent_chain.run(\"what t shirts are available in klarna?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document retrieval: upsert and query basic usage\n",
    "\n",
    "In this walkthrough we will see how to use the retrieval API with a Redis datastore for *semantic search / question-answering*. We will also provide a basic demo showing how to use the \"filter\" function.\n",
    "\n",
    "Before running this notebook you should have already initialized the retrieval API and have it running locally or elsewhere. The full instructions for doing this are found in on the chatgpt-retrieval-plugin page [page](https://github.com/openai/chatgpt-retrieval-plugin#quickstart). Please follow the instructions to start the app with the redis datastore.\n",
    "\n",
    "Additional examples using the search features can be found [here](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/examples/providers/pinecone/semantic-search.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document\n",
    "\n",
    "First we will prepare a collection of documents. From the perspective of the retrieval plugin, a [document](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/models/models.py) this consists\n",
    "of an \"id\", \"text\" and a collection of \"metadata\".\n",
    "\n",
    "The \"metadata\" has \"source\", \"source_id\", \"created_at\", \"url\" and \"author\" fields. Query metadata does not expose the \"url\" field.\n",
    "\n",
    "The \"source\" field is an Enum and can only be one of (\"file\", \"email\" or \"chat\").\n",
    "\n",
    "Text is taken from company SEC 10-K filings which are in the public domain.\n",
    "\n",
    "For demonstration, we will insert some **fake** authors for the documents, see the respective links for the original sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_1 = {\n",
    "    \"id\": \"twtr\",\n",
    "    \"text\": \"\"\"Postponements, suspensions or cancellations of major events, such as sporting events\n",
    "                and music festivals, may lead to people perceiving the content on Twitter as less\n",
    "                relevant or useful or of lower quality, which could negatively affect mDAU growth,\n",
    "                or may reduce monetization opportunities in connection with such events.\"\"\",\n",
    "    \"metadata\" : {\n",
    "        \"source\" : \"file\",\n",
    "        \"source_id\" : \"test:twtr10k\",\n",
    "        \"created_at\": \"2020-12-31\",\n",
    "        \"url\": \"https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm\",\n",
    "        \"author\": 'Elvis Tusk Sr.'        \n",
    "    }\n",
    "}\n",
    "\n",
    "document_2 = {\n",
    "    \"id\": \"tsla\",\n",
    "    \"text\": \"\"\"Because we do not have independent dealer networks, we are responsible for delivering\n",
    "               all of our vehicles to our customers.\"\"\",\n",
    "    \"metadata\" : {\n",
    "        \"source\" : \"file\",\n",
    "        \"source_id\" : \"test:tesla10k\",\n",
    "        \"created_at\": \"2021-12-31\",\n",
    "        \"url\": \"https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm\",\n",
    "        \"author\": 'Elvis Tusk Jr.'        \n",
    "    }     \n",
    "}\n",
    "\n",
    "document_3 = {\n",
    "    \"id\": \"xom\",\n",
    "    \"text\": \"\"\"All practical and economically-viable energy sources will need to be pursued to continue\n",
    "               meeting global energy demand, recognizing the scale and variety of worldwide energy needs\n",
    "               as well as the importance of expanding access to modern energy to promote better standards\n",
    "               of living for billions of people.\"\"\",\n",
    "    \"metadata\" : {\n",
    "        \"source\" : \"file\",\n",
    "        \"source_id\" : \"test:xom10k\",\n",
    "        \"created_at\": \"2020-12-31\",\n",
    "        \"url\": \"https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm\",\n",
    "        \"author\": 'Vape Jordan'        \n",
    "    }     \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to begin indexing (or *upserting*) our `documents`. To make these requests to the retrieval app API, we will need to provide authorization in the form of the `BEARER_TOKEN` we set earlier. We do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\") or \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxOTgzNzE5MjgzNzkzNyIsIm5hbWUiOiJBbmRyemVqIExhcGluc2tpIiwiaWF0IjoxNjg5OTQwMzY5fQ.fko4QOjn6XSWq7nm8QVVjWBeIDmHVfUgr1P0HA7oJuY\"\n",
    "endpoint_url = 'http://0.0.0.0:3333'\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `BEARER_TOKEN` to create our authorization `headers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{endpoint_url}/upsert\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"documents\": [document_1, document_2, document_3]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example filter syntax\n",
    "In our example data we have tagged each companies 10k documents as a source: test:twtr10k, test:tsla10k, and test:xom10k.\n",
    "And we have created **fake** authors of the documents, Elvis Tusk Jr., Elvis Tusk Sr. and Vape Jordan. We will then filter based on these fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TAG Fields\n",
    "\n",
    "source and source_id are \"TAG\" fields, Redis supports a limited [query syntax](https://redis.io/docs/stack/search/reference/tags/) on TAGS, which includes and \"or\" syntax, i.e. \"test:twtr10k|test:tesla10k\" or a ```*``` wildcard to match a prefix.\n",
    "\n",
    "In this example we have only two documents that match the filter so only two documents will show.\n",
    "\n",
    "Gotcha: There cannot be a space between the bar \"|\", i.e. \"test:twtr10k|test:tesla10k\" is valid, \"test:twtr10k | test:tesla10k\" is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'How does Tesla deliver cars?',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.8145653365811582},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7245437859883449},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7000223454611559}]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"How does Tesla deliver cars?\",\n",
    "    \"filter\": {\"source_id\": \"test:twtr10k|test:tesla10k\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use a wild card to filter by prefix. There are three documents matching this filter so three results will be printed.\n",
    "\n",
    "Gotcha, only prefix filtering is supported for redis TAGS, i.e. \"test*\" is valid, where as \"te\\*t\\*\" is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'I want information related to car dealerships.',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.795724915277784},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7079306278833792},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.6948336097185607}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"I want information related to car dealerships.\",\n",
    "    \"filter\": {\"source_id\": \"test:*\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last example we filter by the \"author\" field. The author field is a TextField, and so we have more options for filtering, \n",
    "see [here](https://redis.io/docs/stack/search/reference/query_syntax/) for a complete set of examples.\n",
    "\n",
    "We can select by a specific author, here we only expect to return a single result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'I want information related to car dealerships.',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.795724915277784},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7079306278833792},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.6948336097185607}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"I want information related to car dealerships.\",\n",
    "    \"filter\": {\"source_id\": \"test:*\", \"author\": \"Vape Jordan\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the negation \"-\" to select all documents, except those published by an author called Elvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'I want information related to car dealerships.',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.795724915277784},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7079306278833792},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.6948336097185607}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"I want information related to car dealerships.\",\n",
    "    \"filter\": {\"source_id\": \"test:*\", \"author\": \"-Elvis\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last example we filter two of the authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'query': 'I want information related to car dealerships.',\n",
       "   'results': [{'id': 'tsla_0',\n",
       "     'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:tesla10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',\n",
       "      'created_at': '2021-12-31',\n",
       "      'author': 'Elvis Tusk Jr.',\n",
       "      'document_id': 'tsla'},\n",
       "     'embedding': None,\n",
       "     'score': 0.795724915277784},\n",
       "    {'id': 'twtr_0',\n",
       "     'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:twtr10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Elvis Tusk Sr.',\n",
       "      'document_id': 'twtr'},\n",
       "     'embedding': None,\n",
       "     'score': 0.7079306278833792},\n",
       "    {'id': 'xom_0',\n",
       "     'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',\n",
       "     'metadata': {'source': 'file',\n",
       "      'source_id': 'test:xom10k',\n",
       "      'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',\n",
       "      'created_at': '2020-12-31',\n",
       "      'author': 'Vape Jordan',\n",
       "      'document_id': 'xom'},\n",
       "     'embedding': None,\n",
       "     'score': 0.6948336097185607}]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": \"I want information related to car dealerships.\",\n",
    "    \"filter\": {\"source_id\": \"test:*\", \"author\": \"Elvis*Jr.|Vape\"},\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{endpoint_url}/query\",\n",
    "    headers=headers,\n",
    "    json={\n",
    "        \"queries\": [query]\n",
    "    }\n",
    ")\n",
    "response.raise_for_status()\n",
    "\n",
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "1979a773a5778de9a5fa593a629dff0ab3c80c2563810d3e6a8dfb123dc01c7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
